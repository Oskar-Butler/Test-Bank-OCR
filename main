# ============================
# cell 1: install dependencies into the current Python interpreter
# ============================
import sys, subprocess, importlib

# --- Core dependencies (excluding storage for now to control version) ---
packages = [
    "google-cloud-documentai",
    "google-cloud-secret-manager",
    "google-genai",
    "pandas",
    "requests",
    "pypdf",     # for PDF page count
    "Pillow"     # for image dimensions
]

# Install core packages
subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade"] + packages)

# --- Fix known dependency conflict ---
# google-cloud-aiplatform requires google-cloud-storage <3.0.0
subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade", "google-cloud-storage<3.0.0,>=1.32.0"])

print("✅ Installed compatible Google Cloud + OCR packages to:", sys.executable)

# --- Verify key imports ---
for pkg in ["google.cloud.documentai_v1", "google.cloud.storage", "pandas", "requests", "pypdf", "PIL"]:
    try:
        importlib.import_module(pkg)
        print(f"✅ {pkg} imported successfully.")
    except ImportError:
        print(f"❌ {pkg} failed to import.")

# --- Optional: check versions ---
try:
    import google.cloud.storage, google.cloud.documentai_v1
    print("\nVersion check:")
    print(" - google-cloud-storage:", google.cloud.storage.__version__)
    print(" - google-cloud-documentai:", google.cloud.documentai_v1.__version__)
except Exception as e:
    print("⚠️ Version check skipped:", e)

import logging
# Suppress pypdf warnings
logging.getLogger("pypdf._reader").setLevel(logging.ERROR)

print("✅ Imports successful")

# ============================
# IMPORTS
# ============================
import os, io, json, time, random, math, re
import pandas as pd
import requests
from pypdf import PdfReader
from PIL import Image
from tqdm import tqdm
from google.cloud import documentai_v1 as documentai
from google.api_core.client_options import ClientOptions
from google.cloud import storage, secretmanager
from google import genai

print("✅ Imports successful")

# ============================
# CONFIG
# ============================
PROJECT_ID = "75850893897"
LOCATION = "eu"
PROCESSOR_ID = "ca36b8780a644f9e"
EXCEL_GCS_PATH = "gs://ocr-bank-extractor-bucket/AI Task.xlsx"
EXPORT_BUCKET_NAME = "bank_ocr_output"
FILE_OVERWRITE_NAME = "bank_ocr_output.csv"
SECRET_NAME = "API_KEY"
GEMINI_MODEL = "gemini-2.5-flash"

# Pre-processing thresholds
MAX_PDF_PAGES = 4
MIN_IMAGE_WIDTH = 400
MIN_IMAGE_HEIGHT = 400
ALLOWED_EXTENSIONS = ['.pdf', '.png', '.jpg', '.jpeg', '.tiff']

# Batch settings
DOCAI_BATCH_SIZE = 100
DOCAI_MAX_BATCH_PAGES = 100
DOCAI_WAIT_SECONDS = 60

GEMINI_BATCH_SIZE = 15
GEMINI_MAX_TOKENS = 6000
GEMINI_WAIT_SECONDS = 15
GEMINI_RETRIES = 3
GEMINI_DELAY = 5.0

print("✅ Config set")

# ============================
# LOAD GEMINI API KEY
# ============================
def load_gemini_key(project_id: str, secret_name: str) -> str:
    env_key = os.environ.get("GEMINI_API_KEY")
    if env_key:
        return env_key

    secret_path = f"projects/{project_id}/secrets/{secret_name}/versions/latest"
    client = secretmanager.SecretManagerServiceClient()
    response = client.access_secret_version(request={"name": secret_path})
    return response.payload.data.decode("utf-8")

gemini_api_key = load_gemini_key(PROJECT_ID, SECRET_NAME)
gemini_client = genai.Client(api_key=gemini_api_key)
print("✅ Gemini client initialized")

# ============================
# INITIALIZE GOOGLE CLIENTS
# ============================
docai_client = documentai.DocumentProcessorServiceClient(
    client_options=ClientOptions(api_endpoint=f"{LOCATION}-documentai.googleapis.com")
)
processor_name = docai_client.processor_path(PROJECT_ID, LOCATION, PROCESSOR_ID)
storage_client = storage.Client()
print("✅ DocumentAI and Storage clients initialized")

# ============================
# HELPER FUNCTIONS
# ============================
def parse_gcs_url(url: str):
    parts = url[5:].split("/", 1)
    if len(parts) != 2:
        raise ValueError(f"GCS URL missing blob path: {url}")
    return parts[0], parts[1]

def should_process_file(url: str):
    try:
        if url.lower().startswith("gs://"):
            bucket_name, blob_name = parse_gcs_url(url)
            blob = storage_client.bucket(bucket_name).blob(blob_name)
            file_bytes = blob.download_as_bytes()
        elif url.lower().startswith("http"):
            r = requests.get(url, timeout=30)
            r.raise_for_status()
            file_bytes = r.content
        else:
            return False, f"Unsupported URL format: {url}", None

        ext = '.' + url.split('.')[-1].lower()
        if ext not in ALLOWED_EXTENSIONS:
            return False, f"File type not allowed: {ext}", file_bytes

        if ext == '.pdf':
            reader = PdfReader(io.BytesIO(file_bytes))
            if len(reader.pages) > MAX_PDF_PAGES:
                return False, f"PDF too long ({len(reader.pages)} pages)", file_bytes
        elif ext in ['.png', '.jpg', '.jpeg', '.tiff']:
            with Image.open(io.BytesIO(file_bytes)) as img:
                w, h = img.size
                if w < MIN_IMAGE_WIDTH or h < MIN_IMAGE_HEIGHT:
                    return False, f"Image too small ({w}x{h})", file_bytes

        return True, "File approved", file_bytes
    except Exception as e:
        return False, f"Failed to download or validate: {e}", None

# ============================
# LOAD EXCEL AND VALIDATE FILES
# ============================
bucket_name, blob_name = parse_gcs_url(EXCEL_GCS_PATH)
blob = storage_client.bucket(bucket_name).blob(blob_name)
df = pd.read_excel(io.BytesIO(blob.download_as_bytes()))
print(f"✅ Excel loaded: {len(df)} rows found\n")

approved_files, rejected_files = [], []

print("--- Validating files ---")
for i, row in df.iterrows():
    url = row['Link']
    can_process, reason, file_bytes = should_process_file(url)
    if can_process:
        approved_files.append((i+1, url, file_bytes))
        print(f"Row {i+1}: ✅ APPROVED -> {reason}")
    else:
        rejected_files.append({"row": i+1, "url": url, "reason": reason})
        print(f"Row {i+1}: ❌ REJECTED -> {reason}")

# Save rejected files
if rejected_files:
    rejected_df = pd.DataFrame(rejected_files)
    storage_client.bucket("ocr-bank-extractor-bucket").blob("rejected_files.csv") \
        .upload_from_string(rejected_df.to_csv(index=False), content_type="text/csv")
    print(f"✅ {len(rejected_files)} rejected files saved to GCS\n")

print(f"✅ {len(approved_files)} files approved for processing\n")

# ============================
# HELPER FUNCTIONS
# ============================
def get_blob_from_gcs_uri(gcs_uri):
    """Return a blob object from a gs:// URI."""
    parts = gcs_uri.replace("gs://", "").split("/", 1)
    bucket_name = parts[0]
    blob_name = parts[1]
    return storage_client.bucket(bucket_name).blob(blob_name)


def extract_text_from_docai_json(document: documentai.Document, doc_json: dict = None) -> str:
    """Return the extracted text from a Document proto, with robust fallbacks."""
    # 1) Prefer canonical top-level text
    try:
        if getattr(document, "text", None):
            return document.text.strip()
    except Exception:
        pass

    # 2) Fallback: scan JSON recursively for text-like fields
    if doc_json:
        found_texts = []

        def _walk_and_collect(o):
            if isinstance(o, dict):
                for k, v in o.items():
                    if k.lower() in ("text", "content", "detected_text", "raw_text") and isinstance(v, str):
                        s = v.strip()
                        if s:
                            found_texts.append(s)
                    else:
                        _walk_and_collect(v)
            elif isinstance(o, list):
                for item in o:
                    _walk_and_collect(item)
            elif isinstance(o, str):
                s = o.strip()
                if s:
                    found_texts.append(s)

        _walk_and_collect(doc_json)
        if found_texts:
            return "\n\n".join(found_texts).strip()

    # 3) Try reconstructing from document pages/tokens
    try:
        all_text = []
        for page in getattr(document, "pages", []) or []:
            for para in getattr(page, "paragraphs", []) or []:
                layout = getattr(para, "layout", None)
                if layout and getattr(layout, "text", None):
                    all_text.append(layout.text)
            for token in getattr(page, "tokens", []) or []:
                if getattr(token, "text", None):
                    all_text.append(token.text)
                elif getattr(token, "layout", None) and getattr(token.layout, "text", None):
                    all_text.append(token.layout.text)
        joined = "\n".join(t.strip() for t in all_text if t and t.strip())
        if joined:
            return joined.strip()
    except Exception:
        pass

    # 4) Nothing found
    return ""


# ============================
# 1. STAGE FILES IN GCS
# ============================
print(f"--- Staging {len(approved_files)} files for batch processing ---")
batch_input_configs = []
staged_file_map = {}
staged_gcs_uris = []

staging_bucket = storage_client.bucket(DOCAI_STAGING_BUCKET)

for idx, url, file_bytes in tqdm(approved_files, desc="Staging files"):
    ext = url.lower().split(".")[-1]
    mime_type = {
        "pdf": "application/pdf",
        "png": "image/png",
        "jpg": "image/jpeg",
        "jpeg": "image/jpeg",
        "tiff": "image/tiff",
    }.get(ext, "application/octet-stream")

    staged_file_name = f"row-{idx}-{uuid.uuid4()}.{ext}"
    blob = staging_bucket.blob(staged_file_name)
    blob.upload_from_string(file_bytes, content_type=mime_type)

    gcs_uri = f"gs://{DOCAI_STAGING_BUCKET}/{staged_file_name}"
    staged_gcs_uris.append(gcs_uri)
    batch_input_configs.append(documentai.GcsDocument(gcs_uri=gcs_uri, mime_type=mime_type))
    staged_file_map[staged_file_name] = {"row": idx, "url": url}

print(f"✅ {len(batch_input_configs)} files staged in gs://{DOCAI_STAGING_BUCKET}/\n")


# ============================
# 2. RUN ASYNCHRONOUS BATCH JOB
# ============================
operation_id = None

if batch_input_configs:
    print(f"--- Starting Document AI batch job for {len(batch_input_configs)} files ---")

    output_config = documentai.DocumentOutputConfig(
        gcs_output_config={"gcs_uri": DOCAI_BATCH_OUTPUT_GCS_PATH}
    )

    request = documentai.BatchProcessRequest(
        name=processor_name,
        input_documents=documentai.BatchDocumentsInputConfig(
            gcs_documents=documentai.GcsDocuments(documents=batch_input_configs)
        ),
        document_output_config=output_config,
    )

    operation = docai_client.batch_process_documents(request)

    # Extract the operation ID
    operation_id = operation.operation.name.split("/")[-1]
    print(f"Job started. Waiting for completion... (Operation: {operation.operation.name})")

    try:
        operation.result(timeout=1800)  # Wait up to 30 mins
        print("✅ Document AI batch job complete.\n")
    except Exception as e:
        print(f"❌ Document AI batch job failed: {e}")
        raise
else:
    print("No files to process.")


# ============================
# 3. PARSE OUTPUT FROM GCS (fixed)
# ============================
invoice_texts = []

if operation_id:
    base_prefix = DOCAI_BATCH_OUTPUT_GCS_PATH.replace("gs://", "").split("/", 1)[1]
    parse_prefix = f"{base_prefix}{operation_id}"
    output_bucket_name = DOCAI_BATCH_OUTPUT_GCS_PATH.split("/")[2]
    output_bucket = storage_client.bucket(output_bucket_name)

    print(f"--- Parsing results from gs://{output_bucket_name}/{parse_prefix}/ ---")
    blobs = list(output_bucket.list_blobs(prefix=parse_prefix))

    if not blobs:
        print(f"⚠️ No JSON results found in prefix '{parse_prefix}'")

    for blob in tqdm(blobs, desc="Parsing results"):
        if not blob.name.endswith(".json"):
            continue

        doc_json_bytes = blob.download_as_string()
        try:
            document = documentai.Document.from_json(doc_json_bytes, ignore_unknown_fields=True)
        except Exception as e:
            print(f"W: Failed to parse {blob.name}: {e}")
            document = None

        # --- FIX: map blob filename back to staged filename ---
        base_name = blob.name.split("/")[-1]  # e.g., row-1-<uuid>-0.json
        staged_file_name = None
        for name in staged_file_map:
            if base_name.startswith(name.rsplit(".", 1)[0]):
                staged_file_name = name
                break

        if not staged_file_name:
            print(f"W: Could not determine original filename for {blob.name}")
            continue

        info = staged_file_map[staged_file_name]
        doc_text = extract_text_from_docai_json(document)
        if doc_text:
            invoice_texts.append({
                "row": info["row"],
                "url": info["url"],
                "text": doc_text,
            })
        else:
            print(f"W: {staged_file_name} (Row {info['row']}) had no extracted text.")

    # ============================
    # CLEANUP
    # ============================
    print("Cleaning up staged files...")
    for gcs_uri in staged_gcs_uris:
        try:
            get_blob_from_gcs_uri(gcs_uri).delete()
        except Exception as e:
            print(f"W: Failed to delete {gcs_uri}: {e}")
    print("✅ Staging cleanup complete.\n")
else:
    print("--- Skipping parsing (no job run). ---")


# ============================
# 4. FINAL SUMMARY
# ============================
print("\n--- Processing Summary ---")
print(f"Total rows in Excel: {len(df)}")
print(f"Approved files: {len(approved_files)}")
print(f"Rejected files: {len(rejected_files)}")
print(f"Documents successfully processed: {len(invoice_texts)}")

# Save extracted text to GCS
if invoice_texts:
    extracted_df = pd.DataFrame(invoice_texts)
    output_blob = storage_client.bucket("ocr-bank-extractor-bucket").blob("extracted_invoices.csv")
    output_blob.upload_from_string(extracted_df.to_csv(index=False), content_type="text/csv")
    print("✅ Extracted texts saved to GCS.")

# ============================
# GEMINI EXTRACTION HELPER
# ============================


# --- Step 1: Settings ---
BATCH_SIZE = GEMINI_BATCH_SIZE           # Max invoices per Gemini call
MAX_TOKENS_PER_BATCH = GEMINI_MAX_TOKENS # Safe token budget per batch
WAIT_SECONDS = GEMINI_WAIT_SECONDS       # Delay between batches (seconds)
RETRIES = GEMINI_RETRIES
DELAY = GEMINI_DELAY

# --- Step 2: Token estimation function ---
def estimate_tokens(text: str) -> int:
    """Rough token estimate: 1 token per word (safe approximation)."""
    return len(text.split())

# --- Step 3: Gemini batch helper ---
def extract_with_gemini_batch(texts: list) -> list:
    """Call Gemini for a batch of invoice texts, retrying if overloaded."""
    all_results = []

    if not texts:
        return all_results

    prompt = (
        "You are a financial document expert.\n"
        "Extract the following fields from each invoice text:\n"
        "- Account Name\n- Sort Code (if any)\n- Account Number\n- IBAN (if any)\n\n"
        "Return a JSON array where each element corresponds to an invoice in the same order, "
        "each element having exact keys: \"Account_Name\", \"Sort_Code\", \"Account_Number\", \"IBAN\".\n\n"
        "Invoices:\n"
    )
    
    for idx, txt in enumerate(texts, start=1):
        prompt += f"\nInvoice {idx}:\n---{txt}---\n"

    for attempt in range(1, RETRIES + 1):
        try:
            response = gemini_client.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt,
                config={"response_mime_type": "application/json"}
            )
            batch_results = json.loads(response.text)

            if isinstance(batch_results, list) and len(batch_results) == len(texts):
                all_results.extend(batch_results)
            else:
                raise ValueError("Unexpected Gemini response format.")

            print(f"✅ Gemini batch processed successfully ({len(texts)} invoices).")
            break

        except Exception as e:
            if "503" in str(e) and attempt < RETRIES:
                wait = DELAY * attempt + random.uniform(0, 3)
                print(f"⚠️ Gemini overloaded (attempt {attempt}/{RETRIES}). Retrying in {wait:.1f}s...")
                time.sleep(wait)
                continue

            print(f"❌ Gemini extraction failed after {attempt} attempts: {e}")
            all_results.extend([{"Account_Name": None, "Sort_Code": None, "Account_Number": None, "IBAN": None} for _ in texts])
            break

    return all_results

# --- Step 4: Safe batch processing with token limits ---
all_results = []
current_batch_texts = []
current_batch_tokens = 0
batch_number = 1

for idx, invoice in enumerate(invoice_texts, start=1):
    text = invoice["text"]  # Extract the string from the dict
    tokens = estimate_tokens(text)
    print(f"Invoice {idx}: ~{tokens} tokens")

    # If adding this invoice exceeds token budget and current batch not empty, process it first
    if current_batch_tokens + tokens > MAX_TOKENS_PER_BATCH and current_batch_texts:
        print(f"\n--- Processing Gemini batch {batch_number} ({len(current_batch_texts)} invoices, ~{current_batch_tokens} tokens) ---")
        start_time = time.time()
        batch_results = extract_with_gemini_batch(current_batch_texts)
        all_results.extend(batch_results)
        print(f"⏱ Batch {batch_number} done in {time.time() - start_time:.1f}s")
        time.sleep(WAIT_SECONDS)
        current_batch_texts = []
        current_batch_tokens = 0
        batch_number += 1

    # Add current invoice to batch (even if it alone exceeds token budget)
    current_batch_texts.append(text)
    current_batch_tokens += tokens

# --- Step 5: Process remaining invoices ---
if current_batch_texts:
    print(f"\n--- Processing final Gemini batch {batch_number} ({len(current_batch_texts)} invoices, ~{current_batch_tokens} tokens) ---")
    start_time = time.time()
    batch_results = extract_with_gemini_batch(current_batch_texts)
    all_results.extend(batch_results)
    print(f"⏱ Final batch done in {time.time() - start_time:.1f}s")

# --- Step 6: Summary ---
success_count = sum(1 for r in all_results if any(r.values()))
failure_count = len(all_results) - success_count
print(f"\n✅ Gemini extraction completed for {len(invoice_texts)} invoices.")
print(f"✅ Successful extractions: {success_count}")
print(f"❌ Failed extractions: {failure_count}")

# ============================
# REGEX VALIDATION / CLEANUP + PREVIEW
# ============================
# ============================
# REGEX VALIDATION / CLEANUP + PREVIEW
# ============================

import re
import pandas as pd

def validate_and_clean_fields(data: dict) -> dict:
    """Clean and validate extracted fields using regex."""
    clean = {}

    # Keep row info if exists
    clean["_row"] = data.get("_row", None)

    # --- Account Name ---
    name = data.get("Account_Name")
    clean["Account_Name"] = re.sub(r"[^A-Za-z0-9\s&'.-]", "", name).strip() if name else None

    # --- Sort Code (UK format) ---
    sort_code = data.get("Sort_Code")
    if sort_code:
        sc_match = re.search(r"\b\d{2}[- ]?\d{2}[- ]?\d{2}\b", str(sort_code))
        clean["Sort_Code"] = re.sub(r"[- ]", "", sc_match.group(0)) if sc_match else None
    else:
        clean["Sort_Code"] = None

    # --- Account Number (UK) ---
    acct = data.get("Account_Number")
    if acct:
        acct_clean = re.sub(r"\D", "", str(acct))
        acct_match = re.search(r"\b\d{6,10}\b", acct_clean)
        clean["Account_Number"] = acct_match.group(0) if acct_match else None
    else:
        clean["Account_Number"] = None

    # --- IBAN ---
    iban = data.get("IBAN")
    if iban:
        iban_clean = re.sub(r"\s+", "", str(iban)).upper()
        iban_match = re.search(r"\b[A-Z]{2}[0-9]{2}[A-Z0-9]{11,30}\b", iban_clean)
        clean["IBAN"] = iban_match.group(0) if iban_match else None
    else:
        clean["IBAN"] = None

    return clean


# ============================
# Step 1: Combine Gemini results with original invoice info
# ============================

try:
    # Ensure previous data exists
    _ = invoice_texts
    _ = all_results

    if len(invoice_texts) != len(all_results):
        raise ValueError(f"Data mismatch: {len(invoice_texts)} texts vs {len(all_results)} results.")

    combined_results = []
    for i, gemini_dict in enumerate(all_results):
        # Start with the invoice metadata (row, url, text)
        base_data = invoice_texts[i].copy()

        # Merge with Gemini extracted fields
        base_data.update(gemini_dict)

        # Standardize row identifier for cleanup
        base_data["_row"] = base_data.pop("row", None)
        combined_results.append(base_data)

    # Step 2: Clean the combined data
    cleaned_results = [validate_and_clean_fields(item) for item in combined_results]

except NameError as e:
    raise NameError(
        f"⚠️ '{e.name}' is not defined. Make sure DocAI and Gemini extraction cells have run first!"
    )

# ============================
# Step 3: Convert to DataFrame for preview
# ============================

df_preview = pd.DataFrame(cleaned_results)

# Display first 10 rows
print("\n✅ Preview of cleaned results (with _row fixed):")
display(df_preview.head(10))


# ============================
# Step 4: Optional flag for completely empty rows
# ============================

cols_to_check = [c for c in df_preview.columns if c != "_row"]
empty_rows = df_preview[df_preview[cols_to_check].isnull().all(axis=1)]

if not empty_rows.empty:
    print(f"\n⚠️ Rows that may need manual review ({len(empty_rows)}):")
    display(empty_rows)


# ============================
# BIGQUERY FORMATTING + EXPORT TO GCS
# ============================

import io
import re
import pandas as pd

# ----------------------------
# Step 1: Format DataFrame for BigQuery
# ----------------------------
def format_for_bigquery(df: pd.DataFrame) -> pd.DataFrame:
    """
    Cleans DataFrame columns for BigQuery:
    - Only column names: remove '.', '/', replace spaces with underscores, uppercase
    - Text fields: convert to string, handle 'nan', strip spaces (don't remove . or /)
    """
    df = df.copy()

    # Clean column names only
    df.columns = [re.sub(r"[./]", "_", str(col)).replace(" ", "_").upper() for col in df.columns]

    # Text fields to clean (without replacing . or /)
    text_cols = [
        "OFFICE_CODE", "SUPPLIER_CODE", "INVOICE_NO_", "JOB_SCHEDULE_ESTIMATE_NO_", "TOTAL", "LINK",
        "ACCOUNT_NAME", "SORT_CODE", "ACCOUNT_NUMBER", "IBAN", 
    ]

    for col in text_cols:
        if col in df.columns:
            # Convert to string, handle 'nan', and strip spaces
            df[col] = df[col].astype(str).replace('nan', '').str.strip()

    return df

# ----------------------------
# Step 2: Export DataFrame to GCS as CSV
# ----------------------------
def export_to_gcs(df: pd.DataFrame, bucket_name: str, file_name: str):
    """
    Export a DataFrame to a Google Cloud Storage bucket as CSV.
    """
    if df.empty:
        print("⚠️ No data to export.")
        return

    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False)
    csv_buffer.seek(0)

    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(file_name)
    blob.upload_from_string(csv_buffer.getvalue(), content_type="text/csv")

    print(f"✅ Exported results to gs://{bucket_name}/{file_name}")


# ----------------------------
# Step 3: Merge cleaned extraction results with original Excel columns
# ----------------------------
def _find_original_column(df, target):
    """Find a column in df that matches target ignoring case, spaces, or underscores."""
    tkey = re.sub(r"[ _./]", "", target).lower()
    for c in df.columns:
        if re.sub(r"[ _./]", "", str(c)).lower() == tkey:
            return c
    return None

# Build cleaned results DataFrame
try:
    cleaned_df = df_preview.copy()
except NameError:
    raise NameError("⚠️ 'df_preview' is not defined. Run the 'REGEX VALIDATION' cell first!")

# Prepare original Excel with a matching _row column
orig_df = df.copy()
orig_df['_row'] = orig_df.index + 1

# Columns to bring from original Excel
orig_wanted = [
    "OFFICE_CODE", "SUPPLIER_CODE", "INVOICE_NO_", "JOB_SCHEDULE_ESTIMATE_NO_", "TOTAL", "LINK"
]

# Select original columns, mapped to desired names
orig_selected = pd.DataFrame({'_row': orig_df['_row']})
for cname in orig_wanted:
    match = _find_original_column(orig_df, cname)
    orig_selected[cname] = orig_df[match] if match else ""

# Ensure _row is integer for merging
cleaned_df['_row'] = pd.to_numeric(cleaned_df['_row'], errors='coerce')
orig_selected['_row'] = pd.to_numeric(orig_selected['_row'], errors='coerce')

# Merge original columns with cleaned extraction
merged = pd.merge(orig_selected, cleaned_df, on='_row', how='left')

# ----------------------------
# Step 4: Final formatting and column ordering
# ----------------------------
desired_cols = [
    "OFFICE_CODE", "SUPPLIER_CODE", "INVOICE_NO_", "JOB_SCHEDULE_ESTIMATE_NO_", "TOTAL", "LINK",
    "ACCOUNT_NAME", "SORT_CODE", "ACCOUNT_NUMBER", "IBAN"
]

# Normalize column names
merged.columns = [re.sub(r"[./]", "_", str(col)).replace(" ", "_").upper() for col in merged.columns]

# Ensure all desired columns exist
for col in desired_cols:
    if col not in merged.columns:
        merged[col] = ""

# Final DataFrame: desired columns first, then any extras
extra_cols = [c for c in merged.columns if c not in desired_cols]
final_cols = desired_cols + extra_cols
final_df = merged.reindex(columns=final_cols)

# ----------------------------
# Step 5: Clean values for BigQuery and export
# ----------------------------
final_df = format_for_bigquery(final_df)

# Preview
print("\n✅ Preview of formatted DataFrame for BigQuery:")
display(final_df.head(10))

# Export
export_to_gcs(final_df, EXPORT_BUCKET_NAME, FILE_OVERWRITE_NAME)


